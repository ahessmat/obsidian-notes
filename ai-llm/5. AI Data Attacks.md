# Introduction to AI Data 
![[Pasted image 20250804194914.png]]

## Data Collection
![[Pasted image 20250804195049.png]]

The process begins with `data collection`, gathering raw information from various sources. This might involve capturing user interactions from web applications as `JSON` logs streamed via messaging queues like `Apache Kafka`, ingesting structured transaction records from `SQL` databases like `PostgreSQL`, pulling sensor readings via `MQTT` from IoT devices, scraping public websites using tools like `Scrapy`, or receiving batch files (`CSV`, `Parquet`) from third parties. The collected data can range from images (`JPEG`) and audio (`WAV`) to complex semi-structured formats. The initial quality and integrity of this collected data profoundly impact all downstream processes.
## Storage
![[Pasted image 20250804195429.png]]
Following collection, data requires `storage`. The choice of technology hinges on the data's structure, volume, and access patterns. Structured data often resides in relational databases (`PostgreSQL`), while semi-structured logs might use `NoSQL` databases (`MongoDB`). For large, diverse datasets, organizations frequently employ `data lakes` built on distributed file systems (`Hadoop HDFS`) or cloud object storage (`AWS S3`, `Azure Blob Storage`). Specialized databases like `InfluxDB` cater to time-series data. Importantly, trained models themselves become stored artifacts, often serialized into formats like Python's `pickle` (`.pkl`), `ONNX`, or framework-specific files (`.pt`, `.pth`, `.safetensors`), each presenting unique security considerations if handled improperly.
## Data Processing
![[Pasted image 20250804195833.png]]
Next, raw data undergoes `data processing and transformation`, as it's rarely suitable for direct model use. This stage employs various libraries and frameworks for cleaning, normalization, and feature engineering. Data cleaning might involve handling missing values using `Pandas` and `scikit-learn`'s `Imputers`. Feature scaling often uses `StandardScaler` or `MinMaxScaler`. `Feature engineering` creates new relevant inputs, such as extracting date components or, for text data, performing tokenization and embedding generation using `NLTK` or `spaCy`. Image data might be augmented using `OpenCV` or `Pillow`. Large datasets often necessitate distributed processing frameworks like `Apache Spark` or `Dask`, with orchestration tools like `Apache Airflow` or `Kubeflow Pipelines` managing these complex workflows. The objective is to prepare a high-quality dataset optimized for the AI task.
## Modeling
![[Pasted image 20250804195922.png]]
The processed data then fuels the `analysis and modeling` stage. Data scientists and ML engineers explore the data, often within interactive environments like `Jupyter Notebooks`, and train models using frameworks such as `scikit-learn`, `TensorFlow`, `Jax`, or `PyTorch`. This iterative process involves selecting algorithms (e.g., `RandomForestClassifier`, CNNs), tuning `hyperparameters` (perhaps using `Optuna`), and validating performance. Cloud platforms like `AWS SageMaker` or `Azure Machine Learning` often provide integrated environments for this lifecycle.
## Deployment
![[Pasted image 20250804200159.png]]
Once trained and validated, a model enters the `deployment` stage, where it's integrated into a production environment to serve predictions. Common patterns include exposing the model as a `REST API` using frameworks like `Flask` or `FastAPI`, often containerized with `Docker` and orchestrated by `Kubernetes`. Alternatively, models might become serverless functions (`AWS Lambda`) or be embedded directly into applications or edge devices (using formats like `TensorFlow Lite`). Securing the deployed model file and its surrounding infrastructure is a key concern here.
## Monitoring and Maintenance
![[Pasted image 20250804200542.png]]
Finally, `monitoring and maintenance` constitute an ongoing stage. Deployed models are continuously observed for operational health using tools like `Prometheus` and `Grafana`, while specialized ML monitoring platforms (`WhyLabs`, `Arize AI`) track `data drift`, `concept drift`, and prediction quality. Feedback from predictions and user interactions is logged and often processed alongside newly collected data to periodically `retrain` the model. This retraining is essential for adapting to changing patterns and maintaining performance but simultaneously creates a significant attack vector. Malicious data introduced via feedback loops or ongoing collection can be incorporated during retraining, enabling `online poisoning` attacks. Orchestration tools like `Airflow` often manage these retraining pipelines, making the security of data flowing into them critical.
# AI Data Attacks
Unlike `evasion attacks` (`manipulating inputs to fool a deployed model`) or privacy `attacks` (`extracting sensitive information from a model`), the attacks covered here `fundamentally undermine the model's integrity by corrupting its foundation`: the data it learns from or the format it's stored in.

![[Pasted image 20250804201514.png]]

* Data Collection
	* Threat: Data poisoning
		* Example: Fake reviews, altered metadata
		* Supports `label flipping` and `feature attacks`
	* Poisoned data infiltrates the training set, corrupting the resulting model
* Storage
	* Threat: traditional data security
		* Theft/tampering of training datasets
		* Potential label modification
		* Replace legitimate model with malicious one
* Data processing
	* Threat: corrupted processed data
		* Mislabeled sentiments, errors in images/features
* Modeling
	* Threat: manifestation of data poisoning.
	* May learn incorrect patterns, exhibit biases, or contain hidden backdoors activated by specific inputs later.
* Deployment
	* If the mechanism loading the model from storage is insecure, an attacker could inject a malicious model file at this point, achieving the same trojan effect or code execution via steganography as compromising the `storage` layer directly.
* Monitoring and Maintenance
	* Continuous data poisoning
	* Gradual corruption of data, degrading quality
# Label Attacks
## Label Flipping
* Directly targets the ground truth info used during model training
* An adversary gains access to a portion of the training dataset and deliberately changes the assigned `labels` (the correct answers or categories) for some data points. The actual features of the data points remain untouched; only their associated class designation is altered.
	* e.g. some images labeled `cat` get altered to be labeled `dog` instead.
* Common goal: degrade model performance