https://academy.hackthebox.com/module/details/292

Follows up **Fundamentals of AI**

You will construct three distinct `AI` models in this module:

1. A `Spam Classifier` to determine whether an SMS message is `spam` or not.
2. A `Network Anomaly Detection Model` designed to identify abnormal or potentially malicious network traffic.
3. A `Malware Classifier` using `byteplots`, which are visual representations of binary data.

# Setup
The module gives some installation instructions, but I prefer:

https://www.anaconda.com/docs/getting-started/miniconda/install#linux

Now we can activate the environment with:

````shell-session
source ~/miniconda3/bin/activate
````


````shell-session
conda activate ai
````

Within the `ai` conda environment we should setup for subsequent work with:

```shell-session
conda install -y jupyter jupyterlab notebook ipykernel 
```

```shell-session
conda install -y numpy scipy pandas scikit-learn matplotlib seaborn transformers datasets tokenizers accelerate evaluate optimum huggingface_hub nltk category_encoders
```

```shell-session
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia
```

```shell-session
pip install requests requests_toolbelt
```


We can subsequently run **JupyterLab** with this command:

````shell-session
jupyter lab
````


# The Data
The exercises make use of a **demo_dataset.csv** file containing network log entries which describe network events and include details like:

- `log_id`: Unique identifier for each log entry.
- `source_ip`: Source IP address for the network event.
- `destination_port`: Destination port number used by the event.
- `protocol`: Network protocol employed (e.g., `TCP`, `TLS`, `SSH`).
- `bytes_transferred`: Total bytes transferred during the event.
- `threat_level`: Indicator of the event's severity. `0` denotes normal traffic, `1` indicates low-threat activity, and `2` signifies a high-threat event.

## Challenges
- The dataset contains a mix of numerical and categorical data.
- Missing values and invalid entries appear in some columns, requiring data cleaning.
- Certain numeric columns may contain non-numeric strings, which must be converted or removed.
- The `threat_level` column includes unknown values (e.g., `?`, `-1`) that must be standardized or addressed during preprocessing.

# Data Preprocessing
First we want to identify all of the entries in our data that have invalid data. From that, we can determine how we want to handle it.

## Dropping Invalid Entries
* Prioritizes data accuracy
* Loss of data does not significantly compromise the overall analysis
* Not always possible/desirable
	* Small dataset
	* Invalid entries constitute a substantial portion

Simply put, this approach removes entries with any invalid data, which assures remaining dataset is clean and free of potentially misleading info.

## Imputing Missing Values
* Basic numeric values can utilize median/mean
* Categorical values can use most frequent value
* Does not consider complex relationships among features
	* Alternative considerations: `KNNImputer` or `IterativeImputer`
* Apply domain knowledge for anything else (e.g. a default IP addr of 0.0.0.0)

Process of replacing missing/invalid values in a dataset with best-guesses.

1. Convert missing/invalid values to a standard `NaN`
	1. `df.replace(values,np.nan,inplace=True)`
2. Replace with best-guesses as categorically appropriate

## Preprocessing text with NLTK
```python
import nltk

# Download the necessary NLTK data files
nltk.download("punkt")    #for tokenization
nltk.download("punkt_tab")
nltk.download("stopwords")    #for removing common words that don't contribute to meaning

print("=== BEFORE ANY PREPROCESSING ===") 
print(df.head(5))
```

### Lowercasing
**Lowercasing text** ensures words a handled equally (i.e. "free" and "Free" are the same).
```python
# Convert all message text to lowercase
df["message"] = df["message"].str.lower()
```
### Remove bad punctuation
**Removing unnecessary punctuation and numbers** simplifies data.
* May want to retain some symbols like $ and ! if important/germane
```python
import re

# Remove non-essential punctuation and numbers, keep useful symbols like $ and !
df["message"] = df["message"].apply(lambda x: re.sub(r"[^a-z\s$!]", "", x))
```
### Tokenization
**Tokenization** divides text into individual words (aka "tokens"). This prepares data for ops like removing stop words and applying stemming.
```python
from nltk.tokenize import word_tokenize

# Split each message into individual tokens
df["message"] = df["message"].apply(word_tokenize)
```
### Removing Stop Words
**Stop words** are common words like `and`, `the`, or `is` that often do not add meaningful context. Removing them reduces noise and focuses the model on the words most likely to help distinguish spam from ham messages. By reducing the number of non-informative tokens, we help the model learn more efficiently.
```python
from nltk.corpus import stopwords

# Define a set of English stop words and remove them from the tokens
stop_words = set(stopwords.words("english"))
df["message"] = df["message"].apply(lambda x: [word for word in x if word not in stop_words])
```
### Stemming
**Stemming** normalizes words by reducing them to their base form (e.g., `running` becomes `run`). This consolidates different forms of the same root word, effectively cutting the vocabulary size and smoothing out the text representation. As a result, the model can better understand the underlying concepts without being distracted by trivial variations in word forms.
```python
from nltk.stem import PorterStemmer

# Stem each token to reduce words to their base form
stemmer = PorterStemmer()
df["message"] = df["message"].apply(lambda x: [stemmer.stem(word) for word in x])
```
### Joining tokens back into a single string
While tokens are useful for manipulation, many machine-learning algorithms and vectorization techniques (e.g., TF-IDF) work best with raw text strings. Rejoining the tokens into a space-separated string restores a format compatible with these methods, allowing the dataset to move seamlessly into the feature extraction phase.
```python
# Rejoin tokens into a single string for feature extraction
df["message"] = df["message"].apply(lambda x: " ".join(x))
```
# Data Transformation
Adjusts data format to improve model ingestion, enhancing model stability, interpretability, and predictive performance.
## Feature Extraction
Since models `cannot directly process raw text data`, they rely on numeric representations—such as counts or frequencies of words—to identify patterns.
### Bag-of-words Model
This technique constructs a vocabulary of unique terms from the dataset and represents each message as a vector of term counts. Each element in the vector corresponds to a term in the vocabulary, and its value indicates how often that term appears in the message. Uses **unigrams** (single words) or **bigrams** (pairs of words)
```python
from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms
vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))

# Fit and transform the message column
X = vectorizer.fit_transform(df["message"])

# Labels (target variable)
y = df["label"].apply(lambda x: 1 if x == "spam" else 0)  # Converting labels to 1 and 0
```
- `min_df=1`: A term must appear in at least one document to be included. While this threshold is set to `1` here, higher values can be used in practice to exclude rare terms.
- `max_df=0.9`: Terms that appear in more than 90% of the documents are excluded, removing overly common words that provide limited differentiation.
- `ngram_range=(1, 2)`: The feature matrix captures individual words and common word pairs by including unigrams and bigrams, potentially improving the model’s ability to detect spam patterns.
After this step, `X` becomes a numerical feature matrix ready to be fed into a classifier, such as **Naive Bayes**.
## Encoding
Converts categorical values into a numeric form
- `OneHotEncoder` for binary indicator features that represent each category separately.
	- ![[data_encoding.webp]]
- `LabelEncoder` for integer codes, though this may imply unintended order.
- `HashingEncoder` or frequency-based methods to handle high-cardinality features and control feature space size.
## Handling Skewed Data
By "skew" we are alluding to data that is unevenly distributed (e.g. a major cluster with a few extreme outliers that stretch out the distribution).

Commonly, this is through `scaling` or `transforming` skewed features (e.g. `log` transform to compress large values more than small ones). After the transformation, the distribution is evener, helping the model treat all data points fairly and reducing the risk of overfitting outliers.
## Data Splitting
Splitting the data into 3 distinct subsets:
- `Training Set`: Used to fit the model. Typically accounts for around 60-80% of the entire dataset.
- `Validation Set`: Used for tuning hyperparameters and model selection. Often around 10-20% of the entire dataset.
- `Test Set`: Used only after all model selections and tuning are complete. Often around 10-20% of the entire dataset.
# Evaluating a Model
How do we know that the model's any good?
## Accuracy
* Measures overall correctness.
- Computed as `(true positives + true negatives) / (all instances)`.
- May be misleading in cases of class imbalance.
	- Accuracy can fail to highlight the model's inability to correctly ID minority class.
	- Say the sample data is 99 true positive and 1 true negative; this would make for an accurate model, but not one that can really ID the 1 true negative well at all.

The proportion of correct predictions out of all predictions made. A model with 0.9950 accuracy makes correct predictions 99.5% of the time.
## Precision
- Reflects quality of positive predictions.
- Computed as `true positives / (true positives + false positives)`.
- High `precision` reduces wasted effort caused by false alarms.
	- However, if the model rarely classifies anything as a true positive, high precision may not mean much
	- Say the sample data labels 100 true positives and 1 false positive, but there's 10k total samples. The model is likely overlooking many true negatives.

`Precision` measures how often the model’s predicted positives are truly positive. For `precision: 0.9949`, when the model labels an instance as positive, it is correct 99.49% of the time.
## Recall
- Reflects completeness of positive detection.
- Computed as `true positives / (true positives + false negatives)`.
- High `recall` reduces the risk of missing critical cases.
	- However, a model with very high `recall` but low `precision` might generate too many false positives.

`Recall` measures the model’s ability to identify all positive instances. For `recall: 0.9950`, the model detects 99.50% of all positives.
## F1-score
- Balances `precision` and `recall`.
- Computed as `2 * (precision * recall) / (precision + recall)`.
- Useful for tasks involving class imbalance.

`F1-score` is the harmonic mean of `precision` and `recall`. For `F1-score: 0.9949`, the metric indicates a near-perfect balance between these two aspects.

# Saving a Model
## joblib
`joblib` is a Python library designed to efficiently serialize and deserialize Python objects, particularly those containing large arrays such as NumPy arrays or scikit-learn models.

```python
import joblib

# Save the trained model to a file for future use
model_filename = 'spam_detection_model.joblib'
joblib.dump(best_model, model_filename)

print(f"Model saved to {model_filename}")
```

When a model, such as a scikit-learn pipeline, is saved with `joblib`, it stores the entire model state including learned parameters and configurations. Later, when the model is reloaded, it will immediately be ready to make predictions as if it had just been trained.

```python
loaded_model = joblib.load(model_filename)
predictions = loaded_model.predict(new_messages)
```
# Algorithms

## Naive Bayes
```python
P(A|B) = (P(B|A) * P(A)) / P(B)
```
Where:

- `P(A|B)` is the probability of event `A` occurring, given that `B` is true.
- `P(B|A)` is the probability of event `B` occurring, given that `A` is true.
- `P(A)` is the prior probability of event `A`.
- `P(B)` is the prior probability of event `B`.

In the context of spam detection, `A` can represent the hypothesis that an email is spam (`Spam`), and `B` can represent the observed features of the email (e.g., words, phrases, etc.).

Refer to https://academy.hackthebox.com/module/292/section/3296 for an example of how this is applied.
### Multinomial Naive Bayes
Well suited for **text classification** due to its probabilistic nature and ability to efficiently handle large, sparse feature sets.
## Random Forests
A `Random Forest` is an ensemble machine-learning algorithm that builds multiple `decision trees` and aggregates their predictions. In classification tasks, each tree votes for a class, and the class receiving the majority votes is chosen. In regression tasks, the final prediction is the average of the individual tree outputs.

By combining the outputs of multiple trees, random forests often generalize better than a single decision tree, reducing `overfitting` and providing robust performance even in high-dimensional feature spaces.

Three key concepts shape the construction of a random forest:

1. `Bootstrapping`: Multiple subsets of the training data are created via sampling with replacement. Each subset trains a separate decision tree.
2. `Tree Construction`: For each tree, a random subset of features is considered at every split, ensuring diversity and reducing correlations among trees.
3. `Voting`: After all trees are trained, classification involves majority voting, while regression involves averaging predictions.

# Spam Classification
Using **Naive Bayes**:
```python
P(Spam|Features) = (P(Features|Spam) * P(Spam)) / P(Features)
```

This can be broken down into:

```python
P(Features|Spam) = P(feature1|Spam) * P(feature2|Spam) * ... * P(featureN|Spam)
```

and

```python
P(Features|Not Spam) = P(feature1|Not Spam) * P(feature2|Not Spam) * ... * P(featureN|Not Spam)
```

