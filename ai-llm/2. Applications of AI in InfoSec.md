https://academy.hackthebox.com/module/details/292

Follows up **Fundamentals of AI**

You will construct three distinct `AI` models in this module:

1. A `Spam Classifier` to determine whether an SMS message is `spam` or not.
2. A `Network Anomaly Detection Model` designed to identify abnormal or potentially malicious network traffic.
3. A `Malware Classifier` using `byteplots`, which are visual representations of binary data.

# Setup
The module gives some installation instructions, but I prefer:

https://www.anaconda.com/docs/getting-started/miniconda/install#linux

Now we can activate the environment with:

````shell-session
source ~/miniconda3/bin/activate
````


````shell-session
conda activate ai
````

Within the `ai` conda environment we should setup for subsequent work with:

```shell-session
conda install -y jupyter jupyterlab notebook ipykernel 
```

```shell-session
conda install -y numpy scipy pandas scikit-learn matplotlib seaborn transformers datasets tokenizers accelerate evaluate optimum huggingface_hub nltk category_encoders
```

```shell-session
conda install -y pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia
```

```shell-session
pip install requests requests_toolbelt
```


We can subsequently run **JupyterLab** with this command:

````shell-session
jupyter lab
````


# The Data
The exercises make use of a **demo_dataset.csv** file containing network log entries which describe network events and include details like:

- `log_id`: Unique identifier for each log entry.
- `source_ip`: Source IP address for the network event.
- `destination_port`: Destination port number used by the event.
- `protocol`: Network protocol employed (e.g., `TCP`, `TLS`, `SSH`).
- `bytes_transferred`: Total bytes transferred during the event.
- `threat_level`: Indicator of the event's severity. `0` denotes normal traffic, `1` indicates low-threat activity, and `2` signifies a high-threat event.

## Challenges
- The dataset contains a mix of numerical and categorical data.
- Missing values and invalid entries appear in some columns, requiring data cleaning.
- Certain numeric columns may contain non-numeric strings, which must be converted or removed.
- The `threat_level` column includes unknown values (e.g., `?`, `-1`) that must be standardized or addressed during preprocessing.

# Data Preprocessing
First we want to identify all of the entries in our data that have invalid data. From that, we can determine how we want to handle it.

## Dropping Invalid Entries
* Prioritizes data accuracy
* Loss of data does not significantly compromise the overall analysis
* Not always possible/desirable
	* Small dataset
	* Invalid entries constitute a substantial portion

Simply put, this approach removes entries with any invalid data, which assures remaining dataset is clean and free of potentially misleading info.

## Imputing Missing Values
* Basic numeric values can utilize median/mean
* Categorical values can use most frequent value
* Does not consider complex relationships among features
	* Alternative considerations: `KNNImputer` or `IterativeImputer`
* Apply domain knowledge for anything else (e.g. a default IP addr of 0.0.0.0)

Process of replacing missing/invalid values in a dataset with best-guesses.

1. Convert missing/invalid values to a standard `NaN`
	1. `df.replace(values,np.nan,inplace=True)`
2. Replace with best-guesses as categorically appropriate

# Data Transformation
Adjusts data format to improve model ingestion, enhancing model stability, interpretability, and predictive performance.
## Encoding
Converts categorical values into a numeric form
- `OneHotEncoder` for binary indicator features that represent each category separately.
	- ![[data_encoding.webp]]
- `LabelEncoder` for integer codes, though this may imply unintended order.
- `HashingEncoder` or frequency-based methods to handle high-cardinality features and control feature space size.
## Handling Skewed Data
By "skew" we are alluding to data that is unevenly distributed (e.g. a major cluster with a few extreme outliers that stretch out the distribution).

Commonly, this is through `scaling` or `transforming` skewed features (e.g. `log` transform to compress large values more than small ones). After the transformation, the distribution is evener, helping the model treat all data points fairly and reducing the risk of overfitting outliers.
## Data Splitting
Splitting the data into 3 distinct subsets:
- `Training Set`: Used to fit the model. Typically accounts for around 60-80% of the entire dataset.
- `Validation Set`: Used for tuning hyperparameters and model selection. Often around 10-20% of the entire dataset.
- `Test Set`: Used only after all model selections and tuning are complete. Often around 10-20% of the entire dataset.
# Evaluating a Model
How do we know that the model's any good?
## Accuracy
* Measures overall correctness.
- Computed as `(true positives + true negatives) / (all instances)`.
- May be misleading in cases of class imbalance.
	- Accuracy can fail to highlight the model's inability to correctly ID minority class.
	- Say the sample data is 99 true positive and 1 true negative; this would make for an accurate model, but not one that can really ID the 1 true negative well at all.

The proportion of correct predictions out of all predictions made. A model with 0.9950 accuracy makes correct predictions 99.5% of the time.
## Precision
- Reflects quality of positive predictions.
- Computed as `true positives / (true positives + false positives)`.
- High `precision` reduces wasted effort caused by false alarms.
	- However, if the model rarely classifies anything as a true positive, high precision may not mean much
	- Say the sample data labels 100 true positives and 1 false positive, but there's 10k total samples. The model is likely overlooking many true negatives.

`Precision` measures how often the model’s predicted positives are truly positive. For `precision: 0.9949`, when the model labels an instance as positive, it is correct 99.49% of the time.
## Recall


